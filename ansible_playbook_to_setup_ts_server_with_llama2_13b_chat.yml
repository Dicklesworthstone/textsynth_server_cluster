---
- name: Deploy and run ts_server with llama2_13b_chat
  hosts: all
  gather_facts: yes
  become: yes
  vars:
    user_home: "/home/ubuntu"
    ts_server_dir: "{{ user_home }}/ts_server_free-2023-07-21"
  tasks:
    - name: Copy bash script to remote host
      copy:
        src: run_ts_server_with_llama2_13b_chat_on_ubuntu.sh
        dest: "{{ user_home }}/run_ts_server_with_llama2_13b_chat_on_ubuntu.sh"
        mode: '0755'
      become_user: ubuntu

    - name: Execute the bash script
      shell: bash {{ user_home }}/run_ts_server_with_llama2_13b_chat_on_ubuntu.sh
      async: 1800 # Allow 30 minutes for completion (in case of slow network while downloading ~7gb model file)
      poll: 30    # Check every 30 seconds for completion
      become_user: ubuntu

    - name: Check if ts_server is running
      shell: pgrep -af ts_server
      register: ts_server_running
      ignore_errors: yes
      become_user: ubuntu

    - name: Check if ts_server is listening on port 8088
      shell: sudo lsof -i -P -n | grep ts_server
      register: ts_server_listening
      ignore_errors: yes
      become_user: ubuntu

    - name: Show log if ts_server is not running or not listening
      shell: tail -n 10 {{ ts_server_dir }}/ts_server.log
      register: ts_server_log
      when: ts_server_running.rc != 0 or ts_server_listening.rc != 0
      become_user: ubuntu

    - name: Print log if ts_server is not running or not listening
      debug:
        msg: "{{ ts_server_log.stdout }}"
      when: ts_server_running.rc != 0 or ts_server_listening.rc != 0

    - name: Clean up temporary script file
      file:
        path: "{{ user_home }}/run_ts_server_with_llama2_13b_chat_on_ubuntu.sh"
        state: absent
      become_user: ubuntu
